{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8ea692d-e57e-41cd-be57-ac342e177a4d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BaseModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mGraphState\u001b[39;00m(\u001b[43mBaseModel\u001b[49m):\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m    Represents the state of the graph processing workflow.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03m        relevant_prompt (Optional[str]): The relevant prompt template for the chosen processing method.\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     choice: \u001b[38;5;28mstr\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'BaseModel' is not defined"
     ]
    }
   ],
   "source": [
    "class GraphState(BaseModel):\n",
    "    \"\"\"\n",
    "    Represents the state of the graph processing workflow.\n",
    "\n",
    "    Attributes:\n",
    "        choice (str): The user's choice of processing method.\n",
    "        input_text (str): The original input text from the user.\n",
    "        extra_instructions (str): Any additional instructions for processing.\n",
    "        username (str): The username of the requester.\n",
    "        dataset (str): The dataset to be used for retrieval.\n",
    "        broadness (str): A parameter controlling the breadth of the search.\n",
    "        selected_choices (Optional[List[str]]): Selected choices for multi-header processing.\n",
    "        word_amounts (Optional[List[int]]): Word count targets for each section.\n",
    "        model (Any): The language model to be used.\n",
    "        context (Optional[str]): Retrieved context for the query.\n",
    "        instructions (Optional[str]): Processed instructions.\n",
    "        question (Optional[str]): The processed question.\n",
    "        result (Optional[str]): The final result of the processing.\n",
    "        relevant_prompt (Optional[str]): The relevant prompt template for the chosen processing method.\n",
    "    \"\"\"\n",
    "    choice: str\n",
    "    input_text: str\n",
    "    extra_instructions: str\n",
    "    username: str\n",
    "    dataset: str\n",
    "    broadness: Optional[int] = None\n",
    "    selected_choices: Optional[List[str]] = None\n",
    "    word_amounts: Optional[List[int]] = None\n",
    "    model: Any\n",
    "    context: Optional[str] = None\n",
    "    instructions: Optional[str] = None\n",
    "    question: Optional[str] = None\n",
    "    result: Optional[str] = None\n",
    "    relevant_prompt: Optional[str] = None\n",
    "    retrieved_docs: Optional[List[str]] = None  # New field to store retrieved documents\n",
    "    relevant_docs: Optional[List[str]] = None\n",
    "    company_name: str\n",
    "\n",
    "\n",
    "def process_multiple_headers(state: GraphState) -> str:\n",
    "    def retrieve_subtopic_docs(sub_topic: str, k: int = RETRIEVE_SUBTOPIC_CHUNKS) -> List[str]:\n",
    "        vectorstore_primary = Chroma(\n",
    "            collection_name=state.dataset,\n",
    "            persist_directory=f\"{CHROMA_FOLDER}/{state.username}\",\n",
    "            embedding_function=embedder,\n",
    "        )\n",
    "        try:\n",
    "            retriever = vectorstore_primary.as_retriever(search_type=\"mmr\", k=k)\n",
    "            docs = retriever.get_relevant_documents(sub_topic)\n",
    "            return [doc.page_content for doc in docs[:k]]\n",
    "        except Exception as e:\n",
    "            log.error(f\"Error getting relevant documents for sub-topic: {e}\")\n",
    "            return []\n",
    "\n",
    "    async def invoke_chain(sub_topic, words):\n",
    "        # Retrieve additional documents specific to the sub-topic\n",
    "        subtopic_docs = await retrieve_subtopic_docs(sub_topic)\n",
    "\n",
    "        # Check relevance of subtopic documents\n",
    "        relevant_subtopic_docs = await check_subtopic_relevance(subtopic_docs, sub_topic, state.model)\n",
    "\n",
    "        # Combine the original context with the relevant sub-topic specific documents\n",
    "        combined_context = state.context + \"\\n\\n\" + \"\\n\\n\".join(relevant_subtopic_docs)\n",
    "\n",
    "        chain_input = {\n",
    "            \"context\": combined_context,\n",
    "            \"extra_instructions\": state.instructions or \"\",\n",
    "            \"question\": state.question or \"\",\n",
    "            \"sub_topic\": sub_topic,\n",
    "            \"word_amounts\": words,\n",
    "            \"except_sub_topics\": \",\".join([choice for choice in (state.selected_choices or []) if choice != sub_topic]),\n",
    "            \"company_name\": state.company_name,\n",
    "        }\n",
    "        prompt = ChatPromptTemplate.from_template(state.relevant_prompt)\n",
    "        chain = (\n",
    "                RunnablePassthrough() | prompt | state.model | StrOutputParser()\n",
    "        )\n",
    "        res = await chain.ainvoke(chain_input)\n",
    "        return f\"{sub_topic}:\\n\\n{res}\"\n",
    "\n",
    "    if state.selected_choices and state.word_amounts:\n",
    "        results = await asyncio.gather(*[invoke_chain(sub_topic, words)\n",
    "                                         for sub_topic, words in zip(state.selected_choices, state.word_amounts)])\n",
    "        return \"\\n\\n\".join(results)\n",
    "    else:\n",
    "        return \"No sub-topics or word amounts provided\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c596d57-70f3-4d00-b51f-bdf149d7df61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
